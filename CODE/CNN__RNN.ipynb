{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CNN__RNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JifuW7sgzmu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFoQ2vcPdgqW"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYtfbGQOtcOt"
      },
      "source": [
        "filepath = \"/content/drive/MyDrive/Dataset-Unicauca-Version2-87Atts.csv\"\n",
        "df = pd.read_csv(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmuAC98G87Hu"
      },
      "source": [
        "df=df.iloc[:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDSabNNDtcUe"
      },
      "source": [
        "feats_importance = ['Destination.IP', 'Destination.Port', 'Source.IP', 'Init_Win_bytes_forward',\n",
        " 'min_seg_size_forward', 'Fwd.Packet.Length.Max', 'Init_Win_bytes_backward', 'Flow.IAT.Max',\n",
        " 'Source.Port', 'Flow.Duration', 'Fwd.Packet.Length.Std', 'Bwd.IAT.Total',\n",
        " 'Avg.Fwd.Segment.Size', 'Fwd.Packets.s', 'Fwd.IAT.Total', 'Fwd.IAT.Max',\n",
        " 'Fwd.Packet.Length.Mean', 'Subflow.Fwd.Bytes', 'Flow.Bytes.s', 'Min.Packet.Length',\n",
        " 'Total.Length.of.Fwd.Packets', 'Bwd.IAT.Max', 'Packet.Length.Variance', 'Bwd.Packets.s',\n",
        " 'Flow.IAT.Mean', 'Fwd.Header.Length', 'act_data_pkt_fwd', 'Max.Packet.Length',\n",
        " 'Flow.Packets.s', 'Flow.IAT.Std', 'Packet.Length.Std', 'Idle.Max',\n",
        " 'Fwd.Header.Length.1', 'Bwd.Packet.Length.Mean', 'Bwd.IAT.Std', 'Fwd.Packet.Length.Min',\n",
        " 'Bwd.Packet.Length.Std', 'Avg.Bwd.Segment.Size', 'Average.Packet.Size', 'Total.Length.of.Bwd.Packets',\n",
        " 'Packet.Length.Mean', 'Fwd.IAT.Mean', 'Fwd.IAT.Std', 'Flow.IAT.Min',\n",
        " 'Bwd.IAT.Mean', 'Bwd.Packet.Length.Max', 'Subflow.Fwd.Packets', 'Total.Fwd.Packets',\n",
        " 'Total.Backward.Packets', 'Bwd.Header.Length', 'Subflow.Bwd.Bytes', 'Subflow.Bwd.Packets', \n",
        " 'Idle.Mean', 'Fwd.IAT.Min', 'Down.Up.Ratio', 'Idle.Min']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G8tQV3Zhfjw"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "number = LabelEncoder()\n",
        "df['Label'] = number.fit_transform(df['Label'].astype(str))\n",
        "df['Flow.ID'] = number.fit_transform(df['Flow.ID'].astype(str))\n",
        "df['Source.IP'] = number.fit_transform(df['Source.IP'].astype(str))\n",
        "df['Destination.IP'] = number.fit_transform(df['Destination.IP'].astype(str))\n",
        "df['Timestamp'] = number.fit_transform(df['Timestamp'].astype(str))\n",
        "df['ProtocolName'] = number.fit_transform(df['ProtocolName'].astype(str))\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIuc-hQJtccS"
      },
      "source": [
        "feats = [x for x in df.columns if x != 'ProtocolName']\n",
        "X = df[feats].astype(float)\n",
        "Y = df['ProtocolName']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9nmIKDStcpy"
      },
      "source": [
        "dummy_y = np_utils.to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWfQ03aPtcmC"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUsuSSbPtca_"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSU1AbIAtcXX"
      },
      "source": [
        "import keras.backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no-zWI_2tcR-"
      },
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBA0HndgaljO"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Input\n",
        "inp =  Input(shape=(86,1))\n",
        "conv = Conv1D(filters=2, kernel_size=2)(inp)\n",
        "pool = MaxPool1D(pool_size=2)(conv)\n",
        "flat = Flatten()(pool)\n",
        "dense = Dense(1)(flat)\n",
        "model = Model(inp, dense)\n",
        "model.compile(loss='mae', optimizer='adam',metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "model.fit(X_train,y_train,epochs = 5,batch_size = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v8Pjrb6uAe_"
      },
      "source": [
        "model.compile(loss='mae', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SPv30e7uAnq"
      },
      "source": [
        "%%time\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMyMWheIuAlr"
      },
      "source": [
        "%%time\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJBoqFO_uAiE"
      },
      "source": [
        "print(\"Accuracy:\")\n",
        "print(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbRA3Wv7uAcE"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Training', 'Testing'], loc='upper left')\n",
        "plt.savefig(\"Accuracy_img.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-13qL6YuV38"
      },
      "source": [
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "plt.title('Model loss') \n",
        "plt.ylabel('Loss') \n",
        "plt.xlabel('Epochs') \n",
        "plt.legend(['Training', 'Testing'], loc='upper left') \n",
        "plt.savefig(\"Loss_img.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codBMJrujaCr"
      },
      "source": [
        "\n",
        "X_train=np.expand_dims(X_train, axis=2)\n",
        "y_train=np.expand_dims(y_train,axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpQL_Ng0jWkV"
      },
      "source": [
        "\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(64, activation='relu', input_shape=(86,1)))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "\n",
        "print('Training...')\n",
        "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
        "print (model.summary())\n",
        "print ('\\n')\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train,y_train, epochs=20, batch_size=1, verbose=1)\n",
        "RNN=model.evaluate(X_train,y_train,verbose=1)[1]*100\n",
        "print(\"RNN Accuracy\",RNN,'%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Multiply\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Concatenate, Add, GRU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "\n",
        "\n",
        "from scipy.signal import butter, filtfilt\n",
        "from pickle import dump\n",
        "\n",
        "from IPython.display import display\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "UXiwVGtMj_37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dnn_model():\n",
        "    \n",
        "    x_input = Input(shape=(X_train.shape[-2:]))\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = Dense(units=128, activation='selu')(x)\n",
        "    \n",
        "    x_output = Dense(units=1)(x)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output, \n",
        "                  name='DNN_Model')\n",
        "    return model"
      ],
      "metadata": {
        "id": "Yt5cbUDog3oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = dnn_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kPtwXLtwkFd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,y_train, epochs=20, batch_size=1, verbose=1)\n",
        "DNN=model.evaluate(X_train,y_train,verbose=1)[1]*100\n",
        "print(\"DNN Accuracy\",RNN,'%')"
      ],
      "metadata": {
        "id": "V8q_PkZXkM_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}